{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment 4**"
      ],
      "metadata": {
        "id": "3p5nSxwg3F32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK 1**"
      ],
      "metadata": {
        "id": "rqBi_74UqRIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highlights and differences between Google's patent and the given code:\n",
        "\n",
        "1. Google's Patent explicitly uses positional encodings which are added to the input embeddings to retain the sequence order information. Whereas the professor's code offers an option (is_pos_emb) to include positional embeddings and uses learnable embeddings for positions.\n",
        "\n",
        "2. The Transformer architecture in the patent applies layer normalization and residual connections around each sub-layer (self-attention and feed-forward neural networks). The normalization is typically applied after the residual connection. This code follows a similar approach with layer normalization and residual connections. However, the placement of normalization (before or after the sub-layer operations) can significantly affect performance.\n",
        "\n",
        "3. The patent mentions a self-attention mechanism in each transformer block's self-attention layer, using queries, keys, andvalues to determine the importance of each input position relative to others. The code implements a multi-head self-attention mechanism in each transformer block, with multiple attention heads operating in parallel."
      ],
      "metadata": {
        "id": "tusZ6kiAqcBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class TransformerBlockLM(nn.Module):\n",
        "    class TransformerBlock(nn.Module):\n",
        "        def __init__(self, head_count, in_size, out_size):\n",
        "            super().__init__()\n",
        "            self.comm = TransformerBlockLM.MultiHeadAttention(head_count=head_count,\n",
        "                                                              in_size=in_size,\n",
        "                                                              out_size=out_size)\n",
        "            self.think = TransformerBlockLM.MLP(embed_size=out_size)\n",
        "\n",
        "        def forward(self, x):\n",
        "            return x + self.think(x + self.comm(x))\n",
        "\n",
        "    class MLP(nn.Module):\n",
        "        # FFNN (embed_size, embed_size*4, embed_size)\n",
        "        def __init__(self, embed_size):\n",
        "            super().__init__()\n",
        "            self.mlp = nn.Sequential(nn.Linear(embed_size, embed_size * 4),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Linear(embed_size * 4, embed_size))\n",
        "            self.layerNorm = nn.LayerNorm(embed_size)\n",
        "\n",
        "        def forward(self, x):  # think\n",
        "            return self.layerNorm(self.mlp(x))  # paper - after\n",
        "            # return self.mlp(self.layerNorm(x)) # alternate - before\n",
        "\n",
        "    class MultiHeadAttention(nn.Module):\n",
        "        \"\"\"\n",
        "        multiple parallel SA heads (communication among words)\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(self, head_count, in_size, out_size):\n",
        "            super().__init__()\n",
        "            self.heads = nn.ModuleList(\n",
        "                TransformerBlockLM.SelfAttentionHead(in_size, out_size // head_count)\n",
        "                for _ in range(head_count)\n",
        "            )\n",
        "            self.layerNorm = nn.LayerNorm(out_size)\n",
        "            # self.proj = nn.Linear(out_size, out_size)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # concat over channel/embeddings_size dimension\n",
        "            return self.layerNorm(torch.cat([head(x) for head in self.heads], dim=-1))  # paper - after\n",
        "            # return torch.cat([head(self.layerNorm(x)) for head in self.heads], dim=-1) # alternate - before\n",
        "            # return self.proj(torch.cat([head(x) for head in self.heads], dim=-1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class SelfAttentionHead(nn.Module):\n",
        "        def __init__(self, in_size, out_size):\n",
        "            \"\"\"\n",
        "            in_size is embed_size\n",
        "            out_size is head_size\n",
        "            \"\"\"\n",
        "            super().__init__()\n",
        "            self.head_size = out_size\n",
        "            self.K = nn.Linear(in_size, self.head_size, bias=False)\n",
        "            self.Q = nn.Linear(in_size, self.head_size, bias=False)\n",
        "            self.V = nn.Linear(in_size, self.head_size, bias=False)\n",
        "            self.attention_weights = None  # To store the last attention weights\n",
        "\n",
        "        def forward(self, x):\n",
        "            keys = self.K(x)\n",
        "            queries = self.Q(x)\n",
        "            # affinities :\n",
        "            # all the queries will dot-product with all the keys\n",
        "            # transpose (swap) second dimension (input_length) with third (head_size)\n",
        "            keys_t = keys.transpose(1, 2)\n",
        "            autocorrs = (queries @ keys_t) * (self.head_size ** -0.5)  # (batch_size x input_length x input_length)\n",
        "            '''\n",
        "            (batch_size x input_length x embed_size) @ (batch_size x embed_size x input_length) ----> (batch_size x input_length x input_length)\n",
        "            '''\n",
        "            autocorrs = torch.tril(autocorrs)\n",
        "            autocorrs = autocorrs.masked_fill(autocorrs == 0, float('-inf'))\n",
        "            autocorrs = torch.softmax(autocorrs, dim=-1)\n",
        "            values = self.V(x)  # (batch_size x input_length x head_size)\n",
        "            out = autocorrs @ values\n",
        "            return out\n",
        "\n",
        "    def __init__(self, batch_size=4,\n",
        "                 input_length=8,\n",
        "                 embed_size=16,\n",
        "                 sa_head_size=8,\n",
        "                 sa_multihead_count=4,\n",
        "                 pos_embed=False,\n",
        "                 include_mlp=False):\n",
        "        super().__init__()\n",
        "        self.blocks = None\n",
        "        self.ffn = None\n",
        "        self.sa_heads = None\n",
        "        # sa_head_size head_size of self-attention module\n",
        "        self.sa_head_size = sa_head_size\n",
        "        self.sa_multihead_count = sa_multihead_count\n",
        "\n",
        "        self.val_data = None\n",
        "        self.train_data = None\n",
        "        self.val_text = None\n",
        "        self.train_text = None\n",
        "        self.K = None\n",
        "        self.linear_sahead_to_vocab = None\n",
        "        self.vocab = None\n",
        "        self.token_embeddings_table = None\n",
        "        self.vocab_size = None\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.vocab_size: int\n",
        "        self.is_pos_emb = pos_embed\n",
        "        self.include_mlp = include_mlp\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # input_length = how many consecutive tokens/chars in one input\n",
        "        self.input_length = input_length\n",
        "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
        "        self.batch_size = batch_size\n",
        "        # embed_size = embedding size\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        self.lm_head = None\n",
        "        self.position_embeddings_table = None\n",
        "\n",
        "    def forward(self, in_ids, target=None):\n",
        "        in_ids_emb = self.token_embeddings_table(in_ids[:, -self.input_length:])\n",
        "        if self.is_pos_emb:\n",
        "            in_ids_pos_emb = self.position_embeddings_table(\n",
        "                torch.arange(in_ids[:, -self.input_length:].shape[1], device=self.device)\n",
        "            )\n",
        "            in_ids_emb = in_ids_emb + in_ids_pos_emb\n",
        "\n",
        "        block_outputs = self.blocks(in_ids_emb)\n",
        "        logits = self.linear_sahead_to_vocab(block_outputs)  # compute\n",
        "\n",
        "        if target is None:\n",
        "            ce_loss = None\n",
        "        else:\n",
        "            batch_size, input_length, vocab_size = logits.shape\n",
        "            logits_ = logits.view(batch_size * input_length, vocab_size)\n",
        "            targets = target.view(batch_size * input_length)\n",
        "            ce_loss = F.cross_entropy(logits_, targets)\n",
        "        return logits, ce_loss\n",
        "\n",
        "    # def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
        "    #     \"\"\"\n",
        "    #     train_iters = how many training iterations\n",
        "    #     eval_iters = how many batches to evaluate to get average performance\n",
        "    #     \"\"\"\n",
        "    #     optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "    #     for iteration in range(train_iters):\n",
        "    #         if iteration % eval_iters == 0:\n",
        "    #             avg_loss = self.eval_loss(eval_iters)\n",
        "    #             print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
        "    #         inputs, targets = self.get_batch(split='train')\n",
        "    #         _, ce_loss = self(inputs, targets)\n",
        "    #         optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
        "    #         ce_loss.backward()  # propagate loss back to each unit in the network\n",
        "    #         optimizer.step()  # update network parameters w.r.t the loss\n",
        "    #     # torch.save(self, 'sa_pos_')\n",
        "\n",
        "    def fit(self, train_iters=100, eval_iters=10, lr=0.0001):\n",
        "      \"\"\"\n",
        "      train_iters = how many training iterations\n",
        "      eval_iters = how many batches to evaluate to get average performance\n",
        "      \"\"\"\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "      final_train_loss = None  # variable to hold the final training loss\n",
        "\n",
        "      for iteration in range(1, train_iters + 1):\n",
        "        inputs, targets = self.get_batch(split='train')\n",
        "        _, ce_loss = self(inputs, targets)\n",
        "        optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
        "        ce_loss.backward()  # propagate loss back to each unit in the network\n",
        "        optimizer.step()  # update network parameters w.r.t the loss\n",
        "        if iteration % eval_iters == 0 or iteration == train_iters:\n",
        "            avg_loss = self.eval_loss(eval_iters)\n",
        "            print(f\"iter {iteration}: train {avg_loss['train']} val {avg_loss['eval']}\")\n",
        "            if iteration == train_iters:  # if it's the last iteration\n",
        "                final_train_loss = avg_loss['train']  # store the final training loss\n",
        "\n",
        "      return final_train_loss  # return the final training loss\n",
        "\n",
        "    def generate(self, context_token_ids, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            token_rep, _ = self(context_token_ids)\n",
        "            last_token_rep = token_rep[:, -1, :]\n",
        "            probs = F.softmax(last_token_rep, dim=1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            context_token_ids = torch.cat((context_token_ids, next_token), dim=1)\n",
        "        output_text = self.decoder(context_token_ids[0].tolist())\n",
        "        return output_text\n",
        "\n",
        "    @torch.no_grad()  # tell torch not to prepare for back-propagation (context manager)\n",
        "    def eval_loss(self, eval_iters):\n",
        "        perf = {}\n",
        "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
        "        self.eval()\n",
        "        for split in ['train', 'eval']:\n",
        "            losses = torch.zeros(eval_iters)\n",
        "            for k in range(eval_iters):\n",
        "                tokens, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
        "                _, ce_loss = self(tokens, targets)  # forward pass\n",
        "                losses[k] = ce_loss.item()  # the value of loss tensor as a standard Python number\n",
        "            perf[split] = losses.mean()\n",
        "        self.train()  # turn-on training mode-\n",
        "        return perf\n",
        "\n",
        "    def prep(self, corpus):\n",
        "        self.vocab = sorted(list(set(corpus)))\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        c2i = {c: i for i, c in\n",
        "               enumerate(self.vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
        "        i2c = {i: c for c, i in c2i.items()}  # integer i to char c map\n",
        "\n",
        "        self.encoder = lambda doc: [c2i[c] for c in doc]\n",
        "        self.decoder = lambda nums: ''.join([i2c[i] for i in nums])\n",
        "\n",
        "        n = len(text)\n",
        "        self.train_text = text[:int(n * 0.9)]\n",
        "        self.val_text = text[int(n * 0.9):]\n",
        "\n",
        "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
        "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
        "\n",
        "        # look-up table for embeddings (vocab_size x embed_size)\n",
        "        # it will be mapping each token id to a vector of embed_size\n",
        "        # a wrapper to store vector representations of each token\n",
        "        self.token_embeddings_table = \\\n",
        "            nn.Embedding(self.vocab_size, self.embed_size).to(self.device)\n",
        "\n",
        "        if self.is_pos_emb:\n",
        "            self.position_embeddings_table = nn.Embedding(self.input_length, self.embed_size).to(self.device)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
        "                                                in_size=self.embed_size,\n",
        "                                                out_size=self.sa_head_size),\n",
        "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
        "                                                in_size=self.embed_size,\n",
        "                                                out_size=self.sa_head_size),\n",
        "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
        "                                                in_size=self.embed_size,\n",
        "                                                out_size=self.sa_head_size),\n",
        "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
        "                                                in_size=self.embed_size,\n",
        "                                                out_size=self.sa_head_size),\n",
        "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
        "                                                in_size=self.embed_size,\n",
        "                                                out_size=self.sa_head_size),\n",
        "            TransformerBlockLM.TransformerBlock(head_count=self.sa_multihead_count,\n",
        "                                                in_size=self.embed_size,\n",
        "                                                out_size=self.sa_head_size),\n",
        "        ).to(self.device)\n",
        "        # linear projection of sa_head output to vocabulary\n",
        "        self.linear_sahead_to_vocab = nn.Linear(self.sa_head_size, self.vocab_size).to(self.device)\n",
        "\n",
        "    def get_batch(self, split='train'):\n",
        "        data = self.train_data if split == 'train' else self.val_data\n",
        "        # get random chunks of length batch_size from data\n",
        "        ix = torch.randint(len(data) - self.input_length,\n",
        "                           (self.batch_size,))\n",
        "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
        "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
        "        inputs_batch = inputs_batch.to(self.device)\n",
        "        targets_batch = targets_batch.to(self.device)\n",
        "        # inputs_batch is\n",
        "        return inputs_batch, targets_batch"
      ],
      "metadata": {
        "id": "zvAfSuqebuOj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'a quick brown fox jumps over the lazy dog.\\n ' \\\n",
        "       'lazy dog and a quick brown fox.\\n' \\\n",
        "       'the dog is lazy and the fox jumps quickly.\\n' \\\n",
        "       'a fox jumps over the dog because he is lazy.\\n' \\\n",
        "       'dog is lazy and fox is brown. she quickly jumps over the lazy dog.'\n",
        "\n",
        "import json\n",
        "with open('config.json', 'r') as config_file:\n",
        "    config = json.load(config_file)\n",
        "\n",
        "# batch_size=64,\n",
        "# input_length=16,\n",
        "# embed_size=128,\n",
        "# sa_multihead_count=8,\n",
        "# sa_head_size=128,\n",
        "# pos_embed=True,\n",
        "# include_mlp=True\n",
        "\n",
        "model = TransformerBlockLM(batch_size=config[\"batch_size\"],\n",
        "                           input_length=config[\"input_length\"],\n",
        "                           embed_size=config[\"embed_size\"],\n",
        "                           sa_multihead_count=config[\"sa_multihead_count\"],\n",
        "                           sa_head_size=config[\"sa_head_size\"],\n",
        "                           pos_embed=config[\"pos_embed\"],\n",
        "                           include_mlp=config[\"include_mlp\"])\n",
        "\n",
        "model = model.to(model.device)\n",
        "model.prep(text)\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
        "input_batch, output_batch = model.get_batch(split='train')\n",
        "_, _ = model(input_batch, output_batch)\n",
        "model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
        "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
        "                                                        dtype=torch.long,\n",
        "                                                        device=model.device),\n",
        "                         max_new_tokens=1000)\n",
        "print(outputs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2m-rLIsbwpA",
        "outputId": "341c80e2-2e41-4c98-f82a-d65fe9b414a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params 1097757\n",
            "iter 1000: train 0.1427946239709854 val 0.1660946011543274\n",
            "iter 2000: train 0.13976311683654785 val 0.09808424860239029\n",
            "iter 3000: train 0.13897739350795746 val 0.11803615093231201\n",
            "iter 4000: train 0.13949252665042877 val 0.1292702704668045\n",
            "\n",
            "a fox jumps over the lazy dog.\n",
            " lazy dog and a quick brown fox jumps over the lazy dog.\n",
            " lazy dog and a quick brown fox jumps over the dog because he is lazy.\n",
            "dog is lazy and fox is brown. she quickly jumps brover the lazy dog.\n",
            " lazy dog and a quick brown fox.\n",
            "the dog is lazy and the fox jumps quickly.\n",
            "a fox jumps over the dog because he is lazy.\n",
            "dog is lazy and fox is brown. she quickly jumps brown. she quickly jumpmps over the dog because he is lazy.\n",
            "dog is lazy and fox is brown. she quickly jumps brown. she quickly jumps brown. she quickly jumpps over the lazy dog.\n",
            " lazy dog and a quick brown fox jumps over the dog because he is lazy.\n",
            "dog is lazy and the fox jumps quickly.\n",
            "a fox jumps over the dog because he is lazy.\n",
            "dog is lazy and the fox jumps quickly.\n",
            "a fox jumps over the dog because he is lazy.\n",
            "dog is lazy and fox is brown. she quickly jumps brown. she quickly jumps brown. she quickly jumps brown. she quickly juickly.\n",
            "a fox jumps over the lazy dog.\n",
            " lazy dog and a quick brown f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('emily_dickonson.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "\n",
        "import json\n",
        "with open('config.json', 'r') as config_file:\n",
        "    config = json.load(config_file)\n",
        "\n",
        "# batch_size=128,\n",
        "# input_length=32,\n",
        "# embed_size=64,\n",
        "# sa_multihead_count=4,\n",
        "# sa_head_size=64,\n",
        "# pos_embed=True,\n",
        "# include_mlp=True)\n",
        "\n",
        "model = TransformerBlockLM(batch_size=config[\"batch_size\"],\n",
        "                           input_length=config[\"input_length\"],\n",
        "                           embed_size=config[\"embed_size\"],\n",
        "                           sa_multihead_count=config[\"sa_multihead_count\"],\n",
        "                           sa_head_size=config[\"sa_head_size\"],\n",
        "                           pos_embed=config[\"pos_embed\"],\n",
        "                           include_mlp=config[\"include_mlp\"])\n",
        "model = model.to(model.device)\n",
        "model.prep(text)\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
        "input_batch, output_batch = model.get_batch(split='train')\n",
        "_, _ = model(input_batch, output_batch)\n",
        "model.fit(train_iters=3000, eval_iters=1000, lr=1e-3)\n",
        "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
        "                                                        dtype=torch.long,\n",
        "                                                        device=model.device),\n",
        "                         max_new_tokens=1000)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89wWAbjd6CxV",
        "outputId": "16901ec2-189f-4cd1-eb0c-12b345a6b63c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params 285902\n",
            "iter 1000: train 1.674307107925415 val 1.7200030088424683\n",
            "iter 2000: train 1.4420382976531982 val 1.7019307613372803\n",
            "iter 3000: train 1.2964651584625244 val 1.7839118242263794\n",
            "\n",
            "\n",
            "Where trembles the riversAs; buttern ecsed,\n",
            "And lungthered at before\n",
            "These nearer 窶能n",
            "Could sets above my soul,\n",
            "The will around,\n",
            "The procecious eternity.\n",
            "\n",
            "The very country full\n",
            "To are usualing\n",
            "Rector above towns the place of could them haved from the face\n",
            "To might any quictious proced,\n",
            "   Emiliber to foot\n",
            "When till the ample to refurout\n",
            "There are! 窶能n",
            "\n",
            "XIV.\n",
            "\n",
            "MY TIME:Then I sa, would the procluded and forgeher bruuttered\n",
            "Tast awake it the land.\n",
            "\n",
            "Night ever gentle keps and was achambear\n",
            "Spince that each content,\n",
            "Was its likes the sand.\n",
            "\n",
            "As if this envy sea, 窶能n",
            "That wrist. It stirrow savans, 窶能n",
            "Still ickind blessed in play;\n",
            "The north as all-micvid eye,\n",
            "That is the offered pentive,\n",
            "Is shut this world be\n",
            "\n",
            "Life! Have as ificent the heaven map,\n",
            "When partakes them through\n",
            "I such an enaste;\n",
            "You on the one\n",
            "\n",
            "The mail witness invento thread,\n",
            "And no was the\n",
            "That shorames creature!\n",
            "He satisure he endeaved thee?\n",
            "Death, they veil\n",
            "I Imported a stribtle flits\n",
            "And still pulumn\n",
            "Behind that pircusial he\n",
            "A lette\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code changes:\n",
        "\n",
        "Configuration Loading: Transitioned to loading training parameters from a configuration file, enhancing the flexibility and ease of tuning model parameters without altering the core script.\n",
        "\n",
        "GPU Utilization Fix: Addressed issues in GPU compatibility and efficiency. Previously, despite GPU checks, inefficiencies and errors occurred when running on GPU. These have been corrected by ensuring all tensors and the model are explicitly assigned to the GPU when available. This adjustment significantly reduced computation time for processing datasets (e.g., \"emily_dickinson.txt\") from 25 minutes to approximately 5 minutes, leveraging available GPU resources more effectively.\n",
        "\n",
        "Fit Function: Fixed the fit function to ensure intuitive tracking, and also introduced variable to capture the final training loss, enhancing performance evaluation."
      ],
      "metadata": {
        "id": "voRL7LH-R0YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TASK 2 : Training the model on Warren Buffet text file:**"
      ],
      "metadata": {
        "id": "6Rx2LEcjTARx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./WarrenBuffet.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# model = TransformerBlockLM(batch_size=128,\n",
        "#                            input_length=32,\n",
        "#                            embed_size=64,\n",
        "#                            sa_multihead_count=4,\n",
        "#                            sa_head_size=64,\n",
        "#                            pos_embed=True,\n",
        "#                            include_mlp=True)\n",
        "\n",
        "model = TransformerBlockLM(batch_size=config[\"batch_size\"],\n",
        "                           input_length=config[\"input_length\"],\n",
        "                           embed_size=config[\"embed_size\"],\n",
        "                           sa_multihead_count=config[\"sa_multihead_count\"],\n",
        "                           sa_head_size=config[\"sa_head_size\"],\n",
        "                           pos_embed=config[\"pos_embed\"],\n",
        "                           include_mlp=config[\"include_mlp\"])\n",
        "\n",
        "model = model.to(model.device)\n",
        "model.prep(text)\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
        "input_batch, output_batch = model.get_batch(split='train')\n",
        "_, _ = model(input_batch, output_batch)\n",
        "model_fit1 = model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
        "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
        "                                                        dtype=torch.long,\n",
        "                                                        device=model.device),\n",
        "                         max_new_tokens=1000)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wi8DiYmM7Fj",
        "outputId": "70038a3c-077e-41a0-cf73-23245f83265a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params 287579\n",
            "iter 1000: train 1.5525161027908325 val 1.6638654470443726\n",
            "iter 2000: train 1.375814437866211 val 1.5402963161468506\n",
            "iter 3000: train 1.2909239530563354 val 1.5073893070220947\n",
            "iter 4000: train 1.2352467775344849 val 1.5040324926376343\n",
            "\n",
            "\n",
            "1980 4,022 \n",
            "\n",
            "6,999 \n",
            "\n",
            "Term of \n",
            "book success. After more of \n",
            "which have other stuggest at Berkshire jurmer-outstring: Sometimes, having told, \n",
            "like than a niqual for house remain economic stocks and attiture to \n",
            "9.5% pellind our stock price needs to have day, May from lupless for our office foreignet a wish approad about 50% of management hority, a big $6 billion. And updrestiging \n",
            "in our noted? \n",
            "\n",
            "In experience this job will be it's cnusting new our ownward set income was no timing to the eye immrona@'s enging, what \n",
            "is the purchase. \n",
            "\n",
            "Paul at a MidAmerican price us of continue its \n",
            "that auto entire unable to pointed with all, sochmonities to take on human as experies uncless potentialized for \n",
            "suggether tiesting declinedness of a smarted by found you. Both when he should. \n",
            "\n",
            "We are seek much into the teceyent of you. Instead, Norformance businesses early pan, and most form the basis then riskooos are from these increasing calculation in 2004-2011 2010 2010 \n",
            "2009 \n",
            "\n",
            "\n",
            "Rults \n",
            "\n",
            "\n",
            "U.K.^eting $\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./WarrenBuffet.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# batch_size=64,\n",
        "# input_length=32,\n",
        "# embed_size=128,\n",
        "# sa_multihead_count=8,\n",
        "# sa_head_size=128,\n",
        "# pos_embed=True,\n",
        "# include_mlp=True\n",
        "\n",
        "model = TransformerBlockLM(batch_size=config[\"batch_size\"],\n",
        "                           input_length=config[\"input_length\"],\n",
        "                           embed_size=config[\"embed_size\"],\n",
        "                           sa_multihead_count=config[\"sa_multihead_count\"],\n",
        "                           sa_head_size=config[\"sa_head_size\"],\n",
        "                           pos_embed=config[\"pos_embed\"],\n",
        "                           include_mlp=config[\"include_mlp\"])\n",
        "\n",
        "model = model.to(model.device)\n",
        "model.prep(text)\n",
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "print(f'params {sum([np.prod(p.size()) for p in model_parameters])}')\n",
        "input_batch, output_batch = model.get_batch(split='train')\n",
        "_, _ = model(input_batch, output_batch)\n",
        "model_fit2 = model.fit(train_iters=4000, eval_iters=1000, lr=1e-3)\n",
        "outputs = model.generate(context_token_ids=torch.zeros((1, 1),\n",
        "                                                        dtype=torch.long,\n",
        "                                                        device=model.device),\n",
        "                         max_new_tokens=1000)\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn3Ah9_jeE_b",
        "outputId": "22fed30b-fb61-415a-ab66-c36fde1d0162"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params 287579\n",
            "iter 1000: train 1.5461950302124023 val 1.6538515090942383\n",
            "iter 2000: train 1.3813180923461914 val 1.544770359992981\n",
            "iter 3000: train 1.2938978672027588 val 1.5143314599990845\n",
            "iter 4000: train 1.2377420663833618 val 1.4985711574554443\n",
            "\n",
            "devastand so be tending value. \n",
            "Reinsurer, combined parties (2.1) pagents \n",
            "of the profitably\") \n",
            "will skiding from Fort delucting Berkshire's total-enter made from Berkshire's \n",
            "cloyidication, thought implied for the profit occurs in both what \n",
            "business. All this owns. When misplied transt, if a close terrific from Jack-terms. In the wat \n",
            "can because shortings of stock and now spagers minufoach Jimisclinant \n",
            "\n",
            "1  \n",
            "\n",
            "(in minan 2011 500 as a cost-free the remain over to Chairman \"miles, and I know now take gains half offer companies at lust awbly produce float is require companies less ly in the mistake in family breadtly becompanized how is 99 a.m, 74,709 take over thought 2,2000 partners who convenenic onlashin. \n",
            "\n",
            "Among the sending sector remarkably his this problem whethen along to bid must rathematically, but we financial credit, the \n",
            "\n",
            "case that return had these business selling income consrulop \n",
            "expectat, though, I let's three \n",
            "any people sating pressum conditions to visited taxes Value\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity1 = torch.exp(model_fit1).item()\n",
        "print('Perplexity for model 1 : ', perplexity1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_9ROeepgjBf",
        "outputId": "d1afc397-0c66-4df3-90f8-a8455d769d67"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for model 1 :  3.4392271041870117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity2 = torch.exp(model_fit2).item()\n",
        "print('Perplexity for model 2  : ', perplexity2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2CrVBJ1T8yP",
        "outputId": "8717bf57-37b8-4554-9906-a51ecdadcd0f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for model 2  :  3.426905870437622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance in terms of model perplexity, impressive texts and high impact design choices :\n",
        "\n",
        "The training iterations and loss values show that the modified Transformer model, with **111,5739** parameters is able to perform well.\n",
        "\n",
        "The reported training and validation losses indicate a decently performing model, with the final training loss translating to a perplexity value of approximately 3.4.\n",
        "\n",
        "The perplexity value here signifies that the model has a medium level of certainty in its next-token predictions.\n",
        "\n",
        "Even though the text is not making exact sense in between, it still follows the tone and thematic nuances of the Warren Buffet text. For example, we can observe phrases like \"great achievements in relation to times\", \"Fred & Board's and CEO beber\" and the model also generated numeric values such as ' $6 billion' as well as some percentages.\n",
        "\n",
        "The high-impact design choices contributing to this performance include:\n",
        "\n",
        "**Embedding Size and Self-Attention Heads: **After testing with various configurations, the chosen configuration likely enabled the model to capture and process the complex relationships and nuances in the training data. Larger embedding sizes also allow for more detailed word representations.\n",
        "\n",
        "**Positional Embeddings:** Including positional embeddings was crucial for maintaining the flow and coherence of the generated text over longer sequences.\n",
        "\n",
        "**Extended Training:** The training iterations (4000 iterations with a learning rate of 1e-3) also allowed for a thorough exploration of the parameter space, enabling the model to fine-tune its weights for optimal performance."
      ],
      "metadata": {
        "id": "AXjY3Zfo_rnz"
      }
    }
  ]
}